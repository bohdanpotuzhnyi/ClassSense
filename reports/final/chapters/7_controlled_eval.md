The controlled evaluation was run in short sessions (approximately 15 minutes per participant group) in a BYOD-like setup to approximate realistic classroom constraints.

## Setup

- Student devices used the student UI to submit anonymous feedback during prompted moments.
- Teacher dashboards received the aggregated distribution of class states in real time.
- The physical prototype also provided room-level environment indicators (optional) streamed from a sensor device.

## Execution

To reduce learning effects, condition order was counterbalanced. Participants tested both variants back-to-back:

- **Students:** button UI and slider UI, with repeated prompts during an authentic learning sequence.
- **Teachers:** browser dashboard and external-screen dashboard, while performing a short instructional task.

Immediately after each condition, participants completed a short questionnaire rating usability, disruptiveness/practicality, and adoption intention. At the end, they selected their preferred version and optionally provided short comments.

## Logged data

In addition to questionnaire data, we logged interaction traces for students (timestamps from prompt to completed input) to compare reaction times between the two input variants.
