We conducted a within-subject controlled experiment to compare two interface variants for students (**buttons vs. sliders**) 
and two dashboard configurations for teachers (**browser dashboard vs. external-screen dashboard**). 
A within-subject design was used to reduce inter-individual variability, and the order of conditions was counterbalanced (half A→B, half B→A).

Participants included two secondary school classes of students (n = 26) and practising teachers (n = 7), all working 
in a BYOD setting. Due to hardware constraints, all trials were conducted using a single shared setup consisting of one 
sensor device and one external display.

## Hypotheses

**H1 (students):** A button-based UI will be rated higher in terms of usability, be less disruptive, and be more readily adopted than a slider UI.

**H0 (students):** There is no discernible difference between button and slider UIs with regard to usability, disruptiveness, or user willingness to engage with them.

| Dependent variables                                  | Independent variable                 |
|-----------------------------------------------------|--------------------------------------|
| Perceived usability                                 | Input type (buttons vs. slider)      |
| Perceived disruptiveness of class flow              |                                      |
| Willingness to use                                  |                                      |

**H1 (teachers):** A dashboard on a separate external screen will be rated as more practical and desirable to adopt than a browser-only dashboard.

**H0 (teachers):** The placement of the dashboard had no discernible impact on the perceived practicality, ease of use, or intention to use it.

| Dependent variables            | Independent variable                                      |
|-------------------------------|-----------------------------------------------------------|
| Practicality / workflow fit   | Dashboard location (WebApp in Browser vs. external screen) |
| Perceived usability           |                                                           |
| Adoption intention            |                                                           |

### Measurements

All subjective measures were collected via Microsoft Forms questionnaires using 5-point Likert scales (1 = strongly disagree, 5 = strongly agree). 
For each condition, we measured: Perceived usability; Perceived disruptiveness / practicality in class; Adoption intention

We also collected forced-choice preferences (which version would you use in class?) and logged reaction-time traces for student input.

In addition to questionnaire data, we logged interaction times for students (timestamps from start of interaction
to completed input) to compare reaction times between the two input variants.

### Procedure

**Students (~15 min):**

1.	Introduction (1 min): Clarification that the goal was to compare user interfaces.
2.	Condition 1 (5 min): Students used the first variant (buttons or slider), triggered by 3–5 feedback prompts during an authentic learning sequence.
3.	Questionnaire 1 (2 min): Immediate evaluation of usability, disruptiveness, and willing-ness to use.
4.	Condition 2 (5 min): Students switched to the alternative version using the same interaction prompts.
5.	Questionnaire 2 (2 min): Evaluation of the second version followed by a forced-choice preference question.


**Teachers (~15 min):** 

1.	Introduction (1 min): Short explanation of the dashboard indicators and intended sup-port.
2.	Condition 1 (5 min): Teachers conducted a short instructional task while using the first dashboard configuration.
3.	Questionnaire 1 (2 min): Immediate subjective evaluation.
4.	Condition 2 (5 min): The same task was repeated with the alternative dashboard setup.
5.	Questionnaire 2 (2 min): Second evaluation and final preference question.


The controlled evaluation was run in short sessions (approximately 15 minutes per participant group) in a BYOD-like 
setup to approximate realistic classroom constraints.